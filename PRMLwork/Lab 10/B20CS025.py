# -*- coding: utf-8 -*-
"""B20CS025_Lab10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sntHRz9TCt9fdLVc-INCXy6LG7NfPbi0

LAB 10: SUPPORT VECTOR MACHINE CLASSIFIER FOR SPAM/NON-SPAM Emails
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib import style
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as qda
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as Lda
from sklearn.model_selection import train_test_split as tts
from sklearn.naive_bayes import GaussianNB
from scipy import linalg
import matplotlib as mpl
from sklearn.metrics import accuracy_score,confusion_matrix
import math
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder 
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from numpy.linalg import norm
from tqdm import tqdm
from collections import Counter
from sklearn import *
from scipy.spatial.distance import cdist
import cv2 as cv
import os 
import shutil
from os import listdir
from os.path import isfile, join
sns.set_style("darkgrid")
# %matplotlib inline

df = pd.read_csv('/home/kartik/Desktop/Lab 10/spambase.data')
df

cols = [x for x in range(len(df.columns)-1)]
cols.append('type')
df.columns = cols
df

print("Total number of labels: {}".format(df.shape[0]))
print("Number of spam emails: {}".format(df[df.iloc[:, 57] == 1].shape[0]))
print("Number of non-spam emails: {}".format(df[df.iloc[:, 57]  == 0].shape[0]))

df.isnull().sum()

df.shape

"""Separating features and labels"""

X=df.iloc[:, :-1]
y=df.iloc[:,-1]

"""Standardising the Data

"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X=scaler.fit_transform(X)
X

"""Using the PCA"""

from sklearn.decomposition import PCA
pca = PCA()
pca.fit_transform(X)

pca.get_covariance()

explained_variance=pca.explained_variance_ratio_
explained_variance

with plt.style.context('dark_background'):
    plt.figure(figsize=(6, 4))

    plt.bar(range(57), explained_variance, alpha=0.5, align='center',
            label='individual explained variance')
    plt.ylabel('Explained variance ratio')
    plt.xlabel('Principal components')
    plt.legend(loc='best')
    plt.tight_layout()

"""The first three mainly responsible for high variance data"""

pca=PCA(n_components=3)
X_new=pca.fit_transform(X)
X_new

pca.get_covariance()

explained_variance=pca.explained_variance_ratio_
explained_variance

with plt.style.context('dark_background'):
    plt.figure(figsize=(6, 4))

    plt.bar(range(3), explained_variance, alpha=0.5, align='center',
            label='individual explained variance')
    plt.ylabel('Explained variance ratio')
    plt.xlabel('Principal components')
    plt.legend(loc='best')
    plt.tight_layout()

dummy_X = pd.DataFrame(X_new)
temp_y = np.array(y)
dummy_Y = pd.DataFrame(temp_y)
dummy_Y.columns = ['type']
plot = pd.concat([dummy_X, dummy_Y], axis=1)
sns.pairplot(data=plot, hue='type')

"""Splitting dataset into training set and testing set for better generalisation"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.3, random_state=433)

"""Running SVM with default hyperparameter"""

from sklearn.svm import SVC
from sklearn import metrics
svc=SVC()
svc.fit(X_train,y_train)
y_pred=svc.predict(X_test)
print('Accuracy Score on testing data:')
print(metrics.accuracy_score(y_test,y_pred))

"""Default Linear kernel"""

svc=SVC(kernel='linear')
svc.fit(X_train,y_train)
y_pred=svc.predict(X_test)
print('Accuracy Score on testing data:')
print(metrics.accuracy_score(y_test,y_pred))

"""Default Polynomial kernel with degree = 2"""

svc=SVC(kernel='poly', degree=2)
svc.fit(X_train,y_train)
y_pred=svc.predict(X_test)
print('Accuracy Score on testing data:')
print(metrics.accuracy_score(y_test,y_pred))

"""Polynomial kernel is performing poorly.The reason behind this maybe it is overfitting the training dataset

Default RBF kernel
"""

svc=SVC(kernel='rbf')
svc.fit(X_train,y_train)
y_pred=svc.predict(X_test)
print('Accuracy Score on testing data:')
print(metrics.accuracy_score(y_test,y_pred))

"""We can conclude from above that svm by default uses rbf kernel as a parameter for kernel

Performing K-fold cross validation with different kernels

CV on Linear kernel
"""

from sklearn.model_selection import cross_val_score
svc=SVC(kernel='linear')
scores = cross_val_score(svc, X_new, y, cv=10, scoring='accuracy')
print("On Training dataset different accuracy scores are:")
print(scores)

"""We can see above how the accuracy score is different everytime.This shows that accuracy score depends upon how the datasets got split."""

print("Mean Accuracy score on training Data for Default Linear SVM Kernel with CV = 10")
print(scores.mean())

"""CV on rbf kernel"""

svc=SVC(kernel='rbf')
scores = cross_val_score(svc, X_new, y, cv=10, scoring='accuracy')
print("On Training dataset different accuracy scores are:")
print(scores)

print("Mean Accuracy score on training Data for Default RBF SVM Kernel with CV = 10")
print(scores.mean())

"""CV on Polynomial kernel with degree  = 2"""

svc=SVC(kernel='poly', degree=2)
scores = cross_val_score(svc, X_new, y, cv=10, scoring='accuracy')
print("On Training dataset different accuracy scores are:")
print(scores)

print("Mean Accuracy score on training Data for Default polynomial(deg=2) SVM Kernel with CV = 10")
print(scores.mean())

"""Taking all the values of C and checking out the accuracy scores

LINEAR KERNEL
"""
def maximumLikelihood(X,y):
    dicAns = {}
    dicClasses = {}
    for i in y:
        if i not in dicClasses:
            dicClasses[i] = 1
    for cla in dicClasses:
        meanDic = {}
        xi = X[y == cla].copy()
        meanDic['mean'] = np.mean(xi)
        meanDic['variance'] = np.var(xi)
        dicAns[cla] = meanDic
    return dicAns

"""Let us look into more detail of what is the exact value of C which is giving us a good accuracy score"""

def Prior_Probability(cls):
    d = {}
    for i in cls:
        if i not in d:
            d[i] = 1
        else:
            d[i]+=1
    
    return probability(d, len(cls))

"""Accuracy score is highest for C=0.3

RBF KERNEL
"""

def Prior_Probability(cls):
    d = {}
    for i in cls:
        if i not in d:
            d[i] = 1
        else:
            d[i]+=1
    
    return probability(d, len(cls))

"""Can see that the accuracy is highest for C = 4

Testing for small values of C
"""

def Prior_Probability(cls):
    d = {}
    for i in cls:
        if i not in d:
            d[i] = 1
        else:
            d[i]+=1
    
    return probability(d, len(cls))

"""For C = 4 highest accuracy score is obtained

FOR POLYNOMIAL KERNEL OF DEGREE  = 2
"""

def Prior_Probability(cls):
    d = {}
    for i in cls:
        if i not in d:
            d[i] = 1
        else:
            d[i]+=1
    
    return probability(d, len(cls))

"""for C around 51-60 the accuracy is highest.
Lets have more closer look
"""

(acc_score[50:61])

"""Thus the highest  accuracy is obtained for C = 53

Now performing SVM by taking hyperparameter C=0.3 and kernel as linear
"""

def Prior_Probability(cls):
    d = {}
    for i in cls:
        if i not in d:
            d[i] = 1
        else:
            d[i]+=1
    
    return probability(d, len(cls))

"""Now performing SVM by taking hyperparameter C=4 and kernel as rbf"""

def Prior_Probability(cls):
    d = {}
    for i in cls:
        if i not in d:
            d[i] = 1
        else:
            d[i]+=1
    
    return probability(d, len(cls))

"""Now performing SVM by taking hyperparameter degree=2 and kernel as poly and C = 53"""

def Prior_Probability(cls):
    d = {}
    for i in cls:
        if i not in d:
            d[i] = 1
        else:
            d[i]+=1
    
    return probability(d, len(cls))

