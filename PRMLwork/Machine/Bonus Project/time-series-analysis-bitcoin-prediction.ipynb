{"cells":[{"metadata":{"_uuid":"c610e8253546dc8d9e2ba524a88c56574f66ddf9"},"cell_type":"markdown","source":"## Goal: Fit a time series model to the historical bitcoin daily closing values\n\n1. [Simple Exponential Smoothing](#optparam1)\n2. [Holt's Linear Trend Method](#optparam2)\n3. [Damped Trend Method](#optparam3)\n4. [Taxonomy of Exponential Smoothing Methods](#optparam4)\n\n\n* Importing 'forecast' package which is very useful to model time series data\n* As well as lattice package to use xyplot() function which is a simple function to plot time series in a neat way"},{"metadata":{"_uuid":"0c9a732cd76618cfcd51c8d8a7500dd7f76f5488","_execution_state":"idle","trusted":true},"cell_type":"code","source":"library(forecast)\nlibrary(lattice)\nlist.files(path = \"../input\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2851cd7d579d89a1cae70f34800b4ac9c2b23fbc"},"cell_type":"code","source":"train = read.csv(\"../input/bitcoin_price_Training - Training.csv\", header=T)\ntest = read.csv(\"../input/bitcoin_price_1week_Test - Test.csv\", header=T)\n\n# glimpse of top few rows of train data\n\nhead(train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"012cdab230fe14ea6afce80a28c534333c574a64"},"cell_type":"markdown","source":"* We will predict the daily Closing value time series for future periods.\n* The future data has for validation purposes in in the test data frame"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e38a31d6f2e2928443e7ebdbd0912a27af837d1f"},"cell_type":"code","source":"head(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3e092005bcf02da6ebcb05e3efaec1ef993c236"},"cell_type":"markdown","source":"* Creating **close_tr_df** as a data frame having dates and closing values in USD for **TRAIN** data\n* Creating **close_val_df** as a data frame having dates and closing values in USD for **TEST** data\n\n* We will also use the **mdy()** function from lubridate to convert Date column from factor to a POSIXCt object"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"abd130cd33a88fba4ccdc0757abb80f4eddb9980"},"cell_type":"code","source":"suppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(lubridate))\n\ntrain$Date <- mdy(train$Date)\ntest$Date <- mdy(test$Date)\n\nclose_tr_df <- train[,c(\"Date\", \"Close\")] %>% arrange(Date)\nclose_val_df <- test[,c(\"Date\", \"Close\")] %>% arrange(Date)\ntail(close_tr_df)\nhead(close_val_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8da6995a82cab4f6c760ce8a71b7d0c67446c0f6"},"cell_type":"markdown","source":"* We can see that train data ends at July 31st 2017 while test data starts at August 1st 2017\n* Lets see if we have any missing values"},{"metadata":{"trusted":true,"_uuid":"5d42071b913e68057c8fb489a43dcb553f766416"},"cell_type":"code","source":"table(is.na(close_tr_df))\ntable(is.na(close_val_df))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc4768e53597db8624c4c66aa2ad46b4ffe4d9d3"},"cell_type":"markdown","source":"* No missing values\n'\n* Now using **ts() function and its START= and END=** arguments to create a **ts** object for TRAIN data and VALIDATION (TEST) DATA\n* **ts** object is a time series object. We can do many flexible time series operations directly on this object\n* We also plot the created ts object to see the pattern"},{"metadata":{"trusted":true,"_uuid":"e3b028c98900f83be35cce3dbbd7c13345f7b3c8"},"cell_type":"code","source":"library(repr)\nclose_tr_ts <- ts(close_tr_df$Close,\n                  start = c(as.numeric(format(close_tr_df$Date[1], \"%Y\")), as.numeric(format(close_tr_df$Date[1], \"%j\"))),\n                  end = c(as.numeric(format(close_tr_df$Date[nrow(close_tr_df)], \"%Y\")), as.numeric(format(close_tr_df$Date[nrow(close_tr_df)], \"%j\"))),\n                  frequency = 365)\nclose_val_ts <- ts(close_val_df$Close,\n                  start = c(as.numeric(format(close_val_df$Date[1], \"%Y\")), as.numeric(format(close_val_df$Date[1], \"%j\"))),\n                  end = c(as.numeric(format(close_val_df$Date[nrow(close_val_df)], \"%Y\")), as.numeric(format(close_val_df$Date[nrow(close_val_df)], \"%j\"))),\n                  frequency = 365)\noptions(repr.plot.width=7 , repr.plot.height=7)\nxyplot(close_tr_ts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38258c53b753c029b6cb9567c79bed0e1412308a"},"cell_type":"markdown","source":"***\n* There is a trend starting slowly from late 2015, increasing slowly till mid 2017 and very rapidly thereafter.\n* Seasonality seems to be absent\n* Non-stationary time series.\n\n* **Forecasting approach from initial observations:**\n    1. First we will start off by trying extremely basic models like naive forecast and drift.\n    2. Then we will try out various exponential smoothing models starting with simple exponential smoothing and going on to ets with trend.\n    3. We will also try ARIMA at the end.\n"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"916ad6df2f3732d28a230e4333661dcfe296e6a0"},"cell_type":"code","source":"naive_close <- naive(close_tr_ts, h = nrow(close_val_df))\ndrift_close <- rwf(close_tr_ts, h = nrow(close_val_df), drift = T)\n\ncat(\"1 step Naive model\")\naccuracy(naive_close, close_val_ts)\ncat(\"Drift model i.e. First observation - Last observation\")\naccuracy(drift_close, close_val_ts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b8d1d0f8865f59d795025a6a616921f26896f69"},"cell_type":"markdown","source":"* Both the models are yielding almost similar results.\n* We that see that we are heavily overfitting the train data for both models. This is because the right at the end of the train time series, there is a severe trend and the whole range of the time series has changed.\n* This provides more evidence that exponential smoothing models might provide a reasonable estimate as we can weight the recent observations more in that.\n* Let us try **Exponential Smoothing models**\n<a id=\"optparam1\"></a>\n\n## 1. Simple Exponential Smoothing\n"},{"metadata":{"trusted":true,"_uuid":"c8ea6f6b027d8a4999c38821fb5685c5bcbd44b1","scrolled":true},"cell_type":"code","source":"close_ses <- ses(close_tr_ts)\nsummary(close_ses)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85fb0647d25b43182e49a45cd5b4c075addb134f"},"cell_type":"markdown","source":"* **ses()** function minimizes the SSE to estimate the right value of ALPHA and INITIAL STATE \"L0\"\n* Looking at the summary, the ideal values for ALPHA and L0 are selected as 0.9798 and 134.2529 respectively\n* Lets look at our performance on test data"},{"metadata":{"trusted":true,"_uuid":"ae60544a6e4a583f6eb1590c42cdd6a2cec8e696"},"cell_type":"code","source":"close_ses_preds = data.frame(predict(close_ses, h= nrow(close_val_df)))\naccuracy(close_ses_preds$Point.Forecast, close_val_ts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9cbef5705aefbe774691e64578330d70ad23281e"},"cell_type":"markdown","source":"* The model is still not performing efficiently. RMSE for test set is still $ 352 which was nearly the same as the one obtained by Naive model\n* Factors like optimal **Alpha being 0.97** & **Naive model performing equally well as exponential smoothing** tends to suggest that the **observations in the recent past are more important when predicting Bitcoin Closing values**\n\n* Note that we still haven't accomodated a model component for trend.\n* We can definitely see that trend starts from late 2015 and is pretty much persistent throughout until the end\n* So let us try to accomodate this trend\n<a id=\"optparam2\"></a>\n\n## 2. Holt's linear trend method"},{"metadata":{"trusted":true,"_uuid":"fd12479e41c986c32b5564c382a333edabe77249"},"cell_type":"code","source":"#autoplot(close_ses) +\n#  autolayer(fitted(close_ses), series=\"Fitted\") +\n#  ylab(\"Bitcoin Daily Closing in $\") + xlab(\"Year\")\n\nclose_holt_lt <- holt(close_tr_ts)\nsummary(close_holt_lt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"743990ca19511b172f31a7579d245b03e384d73b"},"cell_type":"code","source":"close_holt_lt_preds = data.frame(predict(close_holt_lt, h= nrow(close_val_df)))\naccuracy(close_holt_lt_preds$Point.Forecast, close_val_ts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d0a7cf60426b3d75ce2c6de98839bd567416668"},"cell_type":"markdown","source":"* RMSE has reduced by $ 30\n\n* In Holt's linear trend method, all the future forecasts are either trended up or trended down constantly.\n* This is not practical for most real-life datasets since some time or the other in the future, the trend will flatten.\n* To accomodate this, there is a method called as DAMPED LINEAR TREND\n* DAMPED LINEAR TREND method is very similar to Holt's method just with an additional dampening parameter which slowly reduces the trend of forecasts.\n* Short term forecasts are trended in this method and long-term forecasts are flattened.\n<a id=\"optparam3\"></a>\n\n## 3. Damped Trend Method"},{"metadata":{"trusted":true,"_uuid":"b88cfaa98a340a868d5c1f64f467956f3de5c8ce"},"cell_type":"code","source":"close_holt_damped <- holt(close_tr_ts, damped=TRUE, phi=0.8)\nclose_holt_damped\n\n# The following code is a plot of Time series with fitted values and is weirdly not working in Kaggle Kernel\n\n#autoplot(close_tr_ts) +\n#  autolayer(close_holt_lt, series=\"Holt's method\", PI=FALSE) +\n#  autolayer(close_holt_damped, series=\"Damped Holt's method\", PI=FALSE) +\n#  ggtitle(\"Forecasts from Holt's method\") + xlab(\"Year\") +\n#  ylab(\"Closing value of Bitcoin in USD\") +\n#  guides(colour=guide_legend(title=\"Forecast\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e973e9c2acd4dc6861b0d214034b9d1b7e9bb4b2"},"cell_type":"code","source":"close_holt_damped_preds = data.frame(predict(close_holt_damped, h= nrow(close_val_df)))\naccuracy(close_holt_damped_preds$Point.Forecast, close_val_ts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e42b2091144c3a1c95de88caba99575afcd6112c"},"cell_type":"markdown","source":"* RMSE did not improve at all for Point.Forecast\n* Interestingly, I tried checking the accuracy with Upper 95% Confidence boundary and the RMSE reduced extensively.\n* This suggests that our model is under-estimating the trend.\n* Lets see if we can correct this in future models"},{"metadata":{"trusted":true,"_uuid":"55d2a839ce51dbc7d89797bedb65fa15ac258988"},"cell_type":"code","source":"accuracy(close_holt_damped_preds$Hi.95, close_val_ts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4f25b1cf4b93e242b53aa0af702acba04734faa"},"cell_type":"markdown","source":"<a id=\"optparam4\"></a>\n\n## 4. Taxonomy of Exponential Smoothing Methods"},{"metadata":{"_uuid":"6fa2c03a3f3bb335272458743ecfa54a3c2bb717"},"cell_type":"markdown","source":"* There are multiple combinations of Exponential Smoothing models available to us.\n* Models with trend, seasonal and level components with either of the components absent/present/additive/multiplicative.\n* **ets()** function takes in a 'ts' object and estimates the best component driven Exponential Smoothing method among several options available"},{"metadata":{"trusted":true,"_uuid":"590c5a270cd4530e5cdfaada94af0ecedc47baaf"},"cell_type":"code","source":"close_ets <- ets(close_tr_ts, damped=FALSE, allow.multiplicative.trend=TRUE)\nclose_ets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"6168404abf2fda67b7b623c57c26d5fb8ead4002"},"cell_type":"code","source":"close_ets_preds = data.frame(predict(close_ets, h= nrow(close_val_df)))\naccuracy(close_ets_preds$Point.Forecast, close_val_ts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f36af140231d6c49fb61782c0574519d7f8d7759"},"cell_type":"markdown","source":"* Finally a BIG improvement w.r.t Point.Forecast\n* Our last best RMSE for point forecast was $ 328\n* ets() chose an Exponential Smoothing model with Multiplicative Level, Multiplicative Trend and Seasonality Absent.\n* Note that we had set the trend dampening to be FALSE, let us set it to be true, although intuitively is should not help because our previous model was under-estimating the values.\n* Before that lets quickly check the RMSE with Upper 95 CL as our forecasts"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"455c44d459c70ff4e4d21c486ec19a57e10f4fd7"},"cell_type":"code","source":"accuracy(close_ets_preds$Hi.95, close_val_ts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20c7a6fbd1d262d5ee61afe87bc56843bcf9d7de"},"cell_type":"markdown","source":"* RMSE has increased for Upper 95 CL. This is good. This means our model has improved and not under-estimated the forecasts.\n* Although, I soon found out that there is still some under estimation as Upper 80 CL gives an RMSE below 200.\n* This might be an indication that in the validation data we have, the values are slightly inflated. If we use Higher 80 CL, it might indicate an overfit of the current validation data.\n* So sticking to Point Forecasts as our actual forecasts will still be a better decision."},{"metadata":{"trusted":true,"_uuid":"58b122ad040283889cbb72d144871b958d72a4ba"},"cell_type":"code","source":"accuracy(close_ets_preds$Hi.80, close_val_ts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"797ddb5b5f8feff1ef09db7cb62e7f9226df7852"},"cell_type":"markdown","source":"* Search for best Component Exponential Smoothing model with **trend DAMPNING**\n* As expected, the model does not help much."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"a449eca8b6a43a26553112a2db92d9f6310345eb"},"cell_type":"code","source":"close_ets_damped <- ets(close_tr_ts, damped=TRUE, allow.multiplicative.trend=TRUE)\nclose_ets_damped_preds = data.frame(predict(close_ets_damped, h= nrow(close_val_df)))\naccuracy(close_ets_damped_preds$Point.Forecast, close_val_ts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"538501296d947e1cf54c6ff7280bf12838e7d595"},"cell_type":"markdown","source":"<a id=\"optparam5\"></a>\n\n## 5. ARIMA Modelling\n\n* So the best results thus far has been ETS(M,M,N) Model - $ 281 RMSE.\n* Let us try to better it using ARIMA models.\n* ARIMA models use the past observations and past errors to create extremely flexible forecasting systems\n* We will use a non-seasonal ARIMA model\n    "},{"metadata":{"trusted":true,"_uuid":"3bc8825021dc4910c198dbccc41d0a8d604dacb2"},"cell_type":"code","source":"(close_arima <- auto.arima(close_tr_ts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56a19c838400163b325f13bd362c2d2a3a95dc2a"},"cell_type":"code","source":"close_arima_preds = close_arima %>% forecast(h= nrow(close_val_df)) %>% data.frame()\naccuracy(close_arima_preds$Point.Forecast, close_val_ts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8caf48a51c25f15c571db25bee28bf6f81c581d2"},"cell_type":"markdown","source":"* We can see that AUTO.ARIMA() selected an ARIMA model with p = 3, d = 2 and q = 0.\n* RMSE is also the best we have had so far, $ 220.\n\n#### Next we will try to better this if it is possible"},{"metadata":{"_uuid":"a068733ec4da7d16e8d283200f4364a5a3276b1a"},"cell_type":"markdown","source":"\n\n## To be continued..."}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}