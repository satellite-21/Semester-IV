# -*- coding: utf-8 -*-
"""PRML Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aGWEtM1i8OpCuAFFuvoS44z-js67an5g

# Music Genre Classification

##Library Import
"""

!pip install pydub

from operator import imod
import numpy as np
from sklearn.cluster import KMeans
import pickle
import joblib
import librosa.display as lplt
import pandas as pd
seed = 12
from tensorflow.keras import layers
import statistics
import seaborn as sns
import tensorflow as tf
from sklearn.decomposition import PCA
import keras as k
import librosa.display as lplt
# tf.random.set_seed(seed)
import matplotlib.pyplot as plt
import IPython
from pydub import AudioSegment
from sklearn.model_selection import train_test_split as tts
# import tensorflow.keras as keras
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
import plotly.express as px
import plotly.graph_objects as go
import json
import os
import math
import librosa
from torch import nn, optim, as_tensor
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from torch.optim import lr_scheduler
from sklearn.preprocessing import LabelEncoder
from torch.nn.init import *
from sklearn.model_selection import train_test_split
from torchvision import transforms, utils, datasets, models
import cv2
from PIL import Image
from pdb import set_trace
import time
import copy
from pathlib import Path
import os
import sys
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from skimage import io, transform
from tqdm import trange, tqdm
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
import csv
import glob
import dlib
import numpy as np
import tensorflow as tf

"""
##Dataset Preparation and Dataset Loading"""

from google.colab import drive
drive.mount('/content/drive')

class Preprocess():
  def __init__(self,folder):
    self.col = ['chroma_stft', 'rms', 'spectral_centroid','spectral_bandwidth', 'spectral_rolloff', 'zero_crossing_rate','harmon', 'plp', 'tempo','mfcc','label']
    self.dataframe = {}
    self.path = folder
    self.dfcol = ['chroma_stft_mean', 'chroma_stft_var', 'rms_mean',
        'rms_var', 'spectral_centroid_mean', 'spectral_centroid_var',
        'spectral_bandwidth_mean', 'spectral_bandwidth_var', 'rolloff_mean',
        'rolloff_var', 'zero_crossing_rate_mean', 'zero_crossing_rate_var',
        'harmony_mean', 'harmony_var', 'plp_mean', 'plp_var', 'tempo','mfcc1_mean', 'mfcc1_var', 'mfcc2_mean', 'mfcc2_var', 'mfcc3_mean',
        'mfcc3_var', 'mfcc4_mean', 'mfcc4_var', 'mfcc5_mean', 'mfcc5_var',
        'mfcc6_mean', 'mfcc6_var', 'mfcc7_mean', 'mfcc7_var', 'mfcc8_mean',
        'mfcc8_var', 'mfcc9_mean', 'mfcc9_var', 'mfcc10_mean', 'mfcc10_var',
        'mfcc11_mean', 'mfcc11_var', 'mfcc12_mean', 'mfcc12_var', 'mfcc13_mean',
        'mfcc13_var', 'mfcc14_mean', 'mfcc14_var', 'mfcc15_mean', 'mfcc15_var',
        'mfcc16_mean', 'mfcc16_var', 'mfcc17_mean', 'mfcc17_var', 'mfcc18_mean',
        'mfcc18_var', 'mfcc19_mean', 'mfcc19_var', 'mfcc20_mean', 'mfcc20_var']
    for i in self.col:
      self.dataframe[i] = []
  
  def extract_features(self):
    for genre in os.listdir(self.path):
      genre_path = self.path +'/'+genre
      for song in os.listdir(genre_path):
        song_path  = genre_path +'/'+ song 
        for i in range(0,30,3):
          t1 = i * 1000 #Works in milliseconds
          t2 = (i+3) * 1000
          newAudio = AudioSegment.from_wav(song_path)
          newAudio = newAudio[t1:t2]
          newAudio.export('newSong.wav', format="wav") #Exports to a wav file in the current path.
          signal, sample_rate = librosa.load('newSong.wav', sr=7000,duration = 3)
          # signal, sample_rate = librosa.load(song_path, sr=5000,duration = 30)
          chroma = librosa.feature.chroma_stft(signal, sr=sample_rate)
          sp_roff = librosa.feature.spectral_rolloff(signal, sr=sample_rate)
          sp_cen = librosa.feature.spectral_centroid(signal, sr=sample_rate)
          sp_bw = librosa.feature.spectral_bandwidth(signal, sr=sample_rate)
          zcr = librosa.feature.zero_crossing_rate(signal)
          rms = librosa.feature.rms(signal)
          mfcc = librosa.feature.mfcc(y=signal, sr=sample_rate)
          hrm = librosa.effects.harmonic(signal)
          tmp = librosa.beat.tempo(signal,sr = sample_rate)
          plp = librosa.beat.plp(signal,sr = sample_rate)
          self.dataframe['chroma_stft'].append(chroma)
          self.dataframe['rms'].append(rms)
          self.dataframe['spectral_centroid'].append(sp_cen)
          self.dataframe['spectral_bandwidth'].append(sp_bw)
          self.dataframe['spectral_rolloff'].append(sp_roff)
          self.dataframe['zero_crossing_rate'].append(zcr)
          self.dataframe['harmon'].append(hrm)
          self.dataframe['plp'].append(plp) 
          self.dataframe['tempo'].append(tmp)
          self.dataframe['mfcc'].append(mfcc)
          self.dataframe['label'].append(song[:-10])
    return

  def dict_extract_to_df(self):
    col = ['chroma_stft', 'rms', 'spectral_centroid','spectral_bandwidth', 'spectral_rolloff', 'zero_crossing_rate','harmon', 'plp']
    X = []
    for i in col:
      mean = []
      var = []
      for j in self.dataframe[i]:
        mean.append(np.mean(j))
        var.append(np.var(j)) 
      X.append(mean)
      X.append(var)
    l = []
    for i in self.dataframe['tempo']:
      l.append(i[0])
    X.append(l)
    X = np.array(X)
    mfc = []
    for m in range(0,20):
      mean = []
      var = []
      for i in self.dataframe['mfcc']:
          var.append(np.var(i[m]))
          mean.append(np.mean(i[m]))
      mfc.append(mean)
      mfc.append(var)
    mfc = np.array(mfc)
    X = np.concatenate([X.T,mfc.T],axis = 1)
    df = pd.DataFrame(X,columns = self.dfcol)
    label = pd.DataFrame(self.dataframe['label'],columns=['label'])
    df = pd.concat((df,label),axis=1)
    return df
  
  def fit(self,X,y=None):
    return self
  
  def transform(self,X):
    self.extract_features()
    df = self.dict_extract_to_df()
    return df

pre = Preprocess('drive/MyDrive/Project/Data/genres_original')
pre.fit('drive/MyDrive/Project/Data/genres_original')
df1 = pre.transform('drive/MyDrive/Project/Data/genres_original')
df1.head()

class genre_classifier():
  def __init__(self,path,model):
    self.path = path
    self.model = model
    self.df = None
    self.col = ['chroma_stft', 'rms', 'spectral_centroid','spectral_bandwidth', 'spectral_rolloff', 'zero_crossing_rate','harmon', 'plp', 'tempo','mfcc']
    self.dataframe = {}
    self.dfcol = ['chroma_stft_mean', 'chroma_stft_var', 'rms_mean',
        'rms_var', 'spectral_centroid_mean', 'spectral_centroid_var',
        'spectral_bandwidth_mean', 'spectral_bandwidth_var', 'rolloff_mean',
        'rolloff_var', 'zero_crossing_rate_mean', 'zero_crossing_rate_var',
        'harmony_mean', 'harmony_var', 'plp_mean', 'plp_var', 'tempo','mfcc1_mean', 'mfcc1_var', 'mfcc2_mean', 'mfcc2_var', 'mfcc3_mean',
        'mfcc3_var', 'mfcc4_mean', 'mfcc4_var', 'mfcc5_mean', 'mfcc5_var',
        'mfcc6_mean', 'mfcc6_var', 'mfcc7_mean', 'mfcc7_var', 'mfcc8_mean',
        'mfcc8_var', 'mfcc9_mean', 'mfcc9_var', 'mfcc10_mean', 'mfcc10_var',
        'mfcc11_mean', 'mfcc11_var', 'mfcc12_mean', 'mfcc12_var', 'mfcc13_mean',
        'mfcc13_var', 'mfcc14_mean', 'mfcc14_var', 'mfcc15_mean', 'mfcc15_var',
        'mfcc16_mean', 'mfcc16_var', 'mfcc17_mean', 'mfcc17_var', 'mfcc18_mean',
        'mfcc18_var', 'mfcc19_mean', 'mfcc19_var', 'mfcc20_mean', 'mfcc20_var']
    for i in self.col:
      self.dataframe[i] = []
  
  def load_file(self):
    audio = AudioSegment.from_wav(self.path)
    sec = audio.duration_seconds
    if(sec>=30):
      t1 = 0 * 1000 #Works in milliseconds
      t2 = (30) * 1000
      audio = audio[t1:t2]
      audio.export(self.path, format="wav")
    for i in range(0,30,3):
      t1 = i * 1000 #Works in milliseconds
      t2 = (i+3) * 1000
      newAudio = AudioSegment.from_wav(self.path)
      newAudio = newAudio[t1:t2]
      newAudio.export('newSong.wav', format="wav") #Exports to a wav file in the current path.
      signal, sample_rate = librosa.load('newSong.wav', sr=7000,duration = 3)
      # signal, sample_rate = librosa.load(song_path, sr=5000,duration = 30)
      chroma = librosa.feature.chroma_stft(signal, sr=sample_rate)
      sp_roff = librosa.feature.spectral_rolloff(signal, sr=sample_rate)
      sp_cen = librosa.feature.spectral_centroid(signal, sr=sample_rate)
      sp_bw = librosa.feature.spectral_bandwidth(signal, sr=sample_rate)
      zcr = librosa.feature.zero_crossing_rate(signal)
      rms = librosa.feature.rms(signal)
      mfcc = librosa.feature.mfcc(y=signal, sr=sample_rate)
      hrm = librosa.effects.harmonic(signal)
      tmp = librosa.beat.tempo(signal,sr = sample_rate)
      plp = librosa.beat.plp(signal,sr = sample_rate)
      self.dataframe['chroma_stft'].append(chroma)
      self.dataframe['rms'].append(rms)
      self.dataframe['spectral_centroid'].append(sp_cen)
      self.dataframe['spectral_bandwidth'].append(sp_bw)
      self.dataframe['spectral_rolloff'].append(sp_roff)
      self.dataframe['zero_crossing_rate'].append(zcr)
      self.dataframe['harmon'].append(hrm)
      self.dataframe['plp'].append(plp) 
      self.dataframe['tempo'].append(tmp)
      self.dataframe['mfcc'].append(mfcc)
    
  def dict_extract_to_df(self):
    col = ['chroma_stft', 'rms', 'spectral_centroid','spectral_bandwidth', 'spectral_rolloff', 'zero_crossing_rate','harmon', 'plp']
    X = []
    for i in col:
      mean = []
      var = []
      for j in self.dataframe[i]:
        mean.append(np.mean(j))
        var.append(np.var(j)) 
      X.append(mean)
      X.append(var)
    l = []
    for i in self.dataframe['tempo']:
      l.append(i[0])
    X.append(l)
    X = np.array(X)
    mfc = []
    for m in range(0,20):
      mean = []
      var = []
      for i in self.dataframe['mfcc']:
          var.append(np.var(i[m]))
          mean.append(np.mean(i[m]))
      mfc.append(mean)
      mfc.append(var)
    mfc = np.array(mfc)
    X = np.concatenate([X.T,mfc.T],axis = 1)
    df = pd.DataFrame(X,columns = self.dfcol)
    return df

  def predict(self):
    self.load_file()
    df = self.dict_extract_to_df()
    self.df = df
    y_pred = self.model.predict(df)
    print(y_pred)
    # y_labels = []
    # y_probs = []
    # for i in y_pred:
    #     y_labels.append(np.argmax(i))
    #     y_probs.append(max(i))
    # # print(y_probs)
    # index = np.argmax(y_probs)
    return statistics.mode(y_pred)

song1 = 'drive/MyDrive/Project/Data/genres_original/jazz/jazz.00098.wav'
gc = genre_classifier(song1,model5)
y_pred = gc.predict()
print(y_pred)

gc.dataframe

def extract_features(song_path):
  columns = ['chroma_stft', 'rms', 'spectral_centroid','spectral_bandwidth', 'spectral_rolloff', 'zero_crossing_rate','harmon', 'plp', 'tempo','mfcc','label']
  dataframe = {}
  for i in columns :
    dataframe[i] = []
  dataframes = []
  for i in range(10):
    dataframes.append(dataframe)
  df = []
  j = 0
  for i in range(0,30,3):
    t1 = i * 1000 #Works in milliseconds
    t2 = (i+3) * 1000
    newAudio = AudioSegment.from_wav(song_path)
    newAudio = newAudio[t1:t2]
    newAudio.export('newSong.wav', format="wav") #Exports to a wav file in the current path.
    signal, sample_rate = librosa.load('newSong.wav', sr=5000,duration = 3)
    # signal, sample_rate = librosa.load(song_path, sr=5000,duration = 30)
    chroma = librosa.feature.chroma_stft(signal, sr=sample_rate)
    sp_roff = librosa.feature.spectral_rolloff(signal, sr=sample_rate)
    sp_cen = librosa.feature.spectral_centroid(signal, sr=sample_rate)
    sp_bw = librosa.feature.spectral_bandwidth(signal, sr=sample_rate)
    zcr = librosa.feature.zero_crossing_rate(signal)
    rms = librosa.feature.rms(signal)
    mfcc = librosa.feature.mfcc(y=signal, sr=sample_rate)
    hrm = librosa.effects.harmonic(signal)
    tmp = librosa.beat.tempo(signal,sr = sample_rate)
    plp = librosa.beat.plp(signal,sr = sample_rate)
    dataframes[j]['chroma_stft'].append(chroma)
    dataframes[j]['rms'].append(rms)
    dataframes[j]['spectral_centroid'].append(sp_cen)
    dataframes[j]['spectral_bandwidth'].append(sp_bw)
    dataframes[j]['spectral_rolloff'].append(sp_roff)
    dataframes[j]['zero_crossing_rate'].append(zcr)
    dataframes[j]['harmon'].append(hrm)
    dataframes[j]['plp'].append(plp) 
    dataframes[j]['tempo'].append(tmp)
    dataframes[j]['mfcc'].append(mfcc)
    dataframes[j]['label'].append(song[:-10])
    j = j + 1
    df.append(dict_extract_to_df(dataframes[j]))
  return pd.concat(df,axis=0)

df.to_csv('keepsafe.csv')

df=pd.read_csv('drive/MyDrive/Project/Data/keepsafe.csv')
df = df.drop(['Unnamed: 0'],axis=1)
df.head()

pca = PCA(n_components=2)
scaler_ = StandardScaler()
df_s = scaler_.fit_transform(df.iloc[:,:-1])
X_pca = pd.DataFrame(pca.fit_transform(df_s),columns=['PCA1','PCA2'])
df_pca = pd.concat([X_pca,df.iloc[:,-1]],axis=1)
df_pca.head()

"""###Spectral Analysis"""

path = 'drive/MyDrive/Project/Data/genres_original'
spectral_dataframe = []
for genre in os.listdir(path):
      genre_path = path +'/'+genre
      for song in os.listdir(genre_path):
        song_path  = genre_path +'/'+ song 
        for i in range(0,30,3):
          t1 = 0 * 1000 #Works in milliseconds
          t2 = (30) * 1000
          audio = AudioSegment.from_wav(path)
          sec = audio.duration_seconds
          if(sec>=30):
            t1 = 0 * 1000 #Works in milliseconds
            t2 = (30) * 1000
            audio = audio[t1:t2]
            audio.export(path, format="wav")
          for i in range(0,30,3):
            t1 = i * 1000 #Works in milliseconds
            t2 = (i+3) * 1000
            newAudio = AudioSegment.from_wav(path)
            newAudio = newAudio[t1:t2]
            newAudio.export('newSong.wav', format="wav")
            sample_rate, samples = wav.read(path)
            f, t, Zxx = signal.stft(samples, fs=1)
            sp = Zxx
            F = 1/5
            n = len(samples)
            l = int((n)/2)
            if l%2 == 0:
              spp = sp[0:l+2]
            else:
              spp = sp[0:l+1]
            ps = [(1/(F*n))*abs(x)**2 for x in spp]
            sps=[2*x for x in ps]
            i = 0
            ff = []
            while i<=F/2:
              ff.append(i)
              i+=F/n
            sps = sps[0:len(ff)]
            sps = np.array(sps)
            ff = np.array(ff)
            T = sps.T
            aperiodic_comp = []
            periodic_comp = []
            for i in range(0,len(T)):    
              fm = FOOOF(verbose=False)
              fm.fit(f,T[i])
              aperiodic_comp.append(fm.get_params('aperiodic_params'))
              periodic_comp.append(fm._peak_fit)
            ap1 = aperiodic_comp
            p1 = periodic_comp
            spectral_dataframe.append(np.mean(np.array(ap1)[:,0]))
            spectral_dataframe.append(np.mean(np.array(ap1)[:,1]))
            spectral_dataframe.append(np.var(np.array(ap1)[:,0]))
            spectral_dataframe.append(np.var(np.array(ap1)[:,1]))
            spectral_dataframe.append(np.mean(np.array(p1)))
            spectral_dataframe.append(np.mean(np.array(p1)))

"""## Loading Given CSV File"""

path = 'drive/MyDrive/Project/Data/features_3_sec.csv'
df2=pd.read_csv(path)
label = LabelEncoder()
df2=df2.drop(['length','filename'],axis=1)
df2['label']=label.fit_transform(df2['label'])
df2.head()

X_train2,X_test2,y_train2,y_test2=tts(df2.iloc[:,:-1],df2.iloc[:,-1],test_size=0.2)
scaler2 = StandardScaler()
X_train2 = pd.DataFrame(scaler2.fit_transform(X_train2), columns=X_train2.columns)
X_test2 = pd.DataFrame(scaler2.transform(X_test2), columns=X_train2.columns)

"""##Dataset Visualization"""

sns.scatterplot(data = df_pca, x="PCA1", y='PCA2', hue="label")

fig = px.scatter(df_pca, x="PCA1", y="PCA2", color="label")
fig.show()

fig = px.bar(df.iloc[:,-1])
fig.show()

# labels = df.iloc[:,:-1]
# for i in list(df.columns)[:-1]:
#   fig = px.box(x = labels, y = df[i],title = i)
#   fig.show()

song = 'drive/MyDrive/Project/Data/genres_original/classical/classical.00098.wav'
audio_data, sr = librosa.load(song)
audio_data, _ = librosa.effects.trim(audio_data)
IPython.display.Audio(audio_data, rate=sr)

plt.figure(figsize=(15,5))
lplt.waveplot(audio_data)
plt.show()

plt.figure(figsize=(16,4))
plt.plot(audio_data[400:600])
plt.show()

n_fft = 2048 # window size
hop_length = 512 # window hop length for STFT

stft = librosa.stft(audio_data, n_fft=n_fft, hop_length=hop_length)
stft_db = librosa.amplitude_to_db(stft, ref=np.max)

plt.figure(figsize=(12,4))
lplt.specshow(stft, sr=sr, x_axis='time', y_axis='hz')
plt.colorbar()
plt.title("Spectrogram with amplitude")
plt.show()

plt.figure(figsize=(12,4))
lplt.specshow(stft_db, sr=sr, x_axis='time', y_axis='log', cmap='cool')
plt.colorbar()
plt.title("Spectrogram with decibel log")
plt.show()

"""##Data Pre-processing"""

scaler = StandardScaler()
X = scaler.fit_transform(df.iloc[:,:-1])

le = LabelEncoder()
y = le.fit_transform(df.iloc[:,-1])

le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
print(le_name_mapping)

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)

"""##Model Training

###Random Forest Classifier
"""

model1 = RandomForestClassifier()
model1.fit(X_train,y_train)
res = model1.predict(X_test)

accuracy_score(y_test,res)

model1.predict_proba(X_test)

print(classification_report(y_test,res))

"""Given Data Set"""

rfc = RandomForestClassifier()
rfc.fit(X_train2,y_train2)
y_pred2=rfc.predict(X_test2)
accuracy_score(y_test2,y_pred2)

"""###Logistic Regressor"""

model2 = LogisticRegression(max_iter=100000)
model2.fit(X_train,y_train)
res = model2.predict(X_test)

accuracy_score(y_test,res)

print(classification_report(y_test,res))

"""Given Data Set"""

lrc = LogisticRegression(max_iter=100000)
lrc.fit(X_train2,y_train2)
y_pred2=lrc.predict(X_test2)
accuracy_score(y_test2,y_pred2)

"""###XGBoost Classifier"""

model3 = XGBClassifier(max_iter=100000)
model3.fit(X_train,y_train)
res3 = model3.predict(X_test)

accuracy_score(y_test,res3)

print(classification_report(y_test,res3))

"""Given Data"""

xgb = XGBClassifier(max_iter=100000)
xgb.fit(X_train2,y_train2)
y_pred2=xgb.predict(X_test2)
accuracy_score(y_test2,y_pred2)

"""###Light GBM Classifier"""

import lightgbm as lgb
model4 = lgb.LGBMClassifier(max_depth=3,num_leaves=9)
model4.fit(X_train,y_train)
res4 = model4.predict(X_test)

accuracy_score(y_test,res4)

res4

print(classification_report(y_test,res4))

lgbm = lgb.LGBMClassifier(max_depth=3,num_leaves=9)
lgbm.fit(X_train2,y_train2)
y_pred2=lgbm.predict(X_test2)
accuracy_score(y_test2,y_pred2)

"""###Support Vector Classifier"""

model5 = SVC(probability = True)
model5.fit(X_train,y_train)
res5 = model5.predict(X_test)

accuracy_score(y_test,res5)

model5.predict_proba(X_test)

print(classification_report(y_test,res5))

"""Given Data"""

svc = SVC(probability=True)
svc.fit(X_train2,y_train2)
y_pred2=svc.predict(X_test2)
accuracy_score(y_test2,y_pred2)

"""###Decision Tree Classifier"""

model = DecisionTreeClassifier()
model.fit(X_train,y_train)
res = model.predict(X_test)

accuracy_score(y_test,res)

print(classification_report(y_test,res))

"""Given Data"""

dtr = DecisionTreeClassifier()
dtr.fit(X_train2,y_train2)
y_pred2=dtr.predict(X_test2)
accuracy_score(y_test2,y_pred2)

"""###CNN

###Model 1
"""

model_1nn = k.models.Sequential([
    k.layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),
    k.layers.Dropout(0.3),
    
    k.layers.Dense(128, activation='relu'),
    k.layers.Dropout(0.3),

    k.layers.Dense(64, activation='relu'),
    k.layers.Dropout(0.3),

    k.layers.Dense(10, activation='softmax'),
])

print(model_1nn.summary())
model_1nn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])
history_1nn = model_1nn.fit(X_train, y_train, epochs=50, batch_size=72, 
                  validation_data = [X_test,y_test],shuffle=False)

fig = go.Figure()
fig.add_trace(go.Scatter(y=history_1nn.history['acc'], x=[i for i in range(0,50)],
                    mode='lines',
                    name='Training Accuracy'))
fig.add_trace(go.Scatter(y=history_1nn.history['val_acc'], x=[i for i in range(0,50)],
                    mode='lines',
                    name='Testing Accuracy'))
fig.update_layout(title='Average High and Low Temperatures in New York',
                   xaxis_title='Month',
                   yaxis_title='Temperature (degrees F)')
fig.show()

fig = go.Figure()
fig.add_trace(go.Scatter(y=history_1nn.history['loss'], x=[i for i in range(0,50)],
                    mode='lines',
                    name='Training Loss'))
fig.add_trace(go.Scatter(y=history_1nn.history['val_loss'], x=[i for i in range(0,50)],
                    mode='lines',
                    name='Validation Loss'))
fig.update_layout(title='Validation and Training Loss',
                   xaxis_title='loss',
                   yaxis_title='epochs')
fig.show()

"""###Model 2"""

model_2nn = k.models.Sequential([
    k.layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)),
    k.layers.Dropout(0.3),
    
    k.layers.Dense(256, activation='relu'),
    k.layers.Dropout(0.3),

    k.layers.Dense(128, activation='relu'),
    k.layers.Dropout(0.3),

    k.layers.Dense(64, activation='relu'),
    k.layers.Dropout(0.3),

    k.layers.Dense(10, activation='softmax'),
])

print(model_2nn.summary())
model_2nn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])
history_2nn = model_2nn.fit(X_train, y_train, epochs=50, batch_size=72, 
                  validation_data = [X_test,y_test],shuffle=False)

fig = go.Figure()
fig.add_trace(go.Scatter(y=history_2nn.history['acc'], x=[i for i in range(0,50)],
                    mode='lines',
                    name='Training Accuracy'))
fig.add_trace(go.Scatter(y=history_2nn.history['val_acc'], x=[i for i in range(0,50)],
                    mode='lines',
                    name='Validation Accuracy'))
fig.update_layout(title='Validation and Training Accuracies',
                   xaxis_title='epochs',
                   yaxis_title='Accuracy')
fig.show()

fig = go.Figure()
fig.add_trace(go.Scatter(y=history_2nn.history['loss'], x=[i for i in range(0,50)],
                    mode='lines',
                    name='Training Loss'))
fig.add_trace(go.Scatter(y=history_2nn.history['val_loss'], x=[i for i in range(0,50)],
                    mode='lines',
                    name='Validation Loss'))
fig.update_layout(title='Validation and Training Loss',
                   xaxis_title='loss',
                   yaxis_title='epochs')
fig.show()

"""###Model 3"""

model_3nn = k.models.Sequential([
    k.layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)),

    k.layers.Dense(256, activation='relu'),

    k.layers.Dense(128, activation='relu'),

    k.layers.Dense(64, activation='relu'),

    k.layers.Dense(10, activation='softmax'),
])

print(model_3nn.summary())
model_3nn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])
history_3nn = model_3nn.fit(X_train, y_train, epochs=50, batch_size=72, 
                  validation_data = [X_test,y_test],shuffle=False)

fig = go.Figure()
fig.add_trace(go.Scatter(y=history_3nn.history['acc'], x=[i for i in range(0,50)],
                    mode='lines',
                    name='Training Accuracy'))
fig.add_trace(go.Scatter(y=history_3nn.history['val_acc'], x=[i for i in range(0,50)],
                    mode='lines',
                    name='Validation Accuracy'))
fig.update_layout(title='Validation and Training Accuracies',
                   xaxis_title='epochs',
                   yaxis_title='Accuracy')
fig.show()

fig = go.Figure()
fig.add_trace(go.Scatter(y=history_3nn.history['loss'], x=[i for i in range(0,50)],
                    mode='lines',
                    name='Training Loss'))
fig.add_trace(go.Scatter(y=history_3nn.history['val_loss'], x=[i for i in range(0,50)],
                    mode='lines',
                    name='Validation Loss'))
fig.update_layout(title='Validation and Training Loss',
                   xaxis_title='loss',
                   yaxis_title='epochs')
fig.show()

"""###Model 4"""

model_4nn = k.models.Sequential([
    k.layers.Dense(1024, activation='relu', input_shape=(X_train.shape[1],)),
    k.layers.Dropout(0.3),
    
    k.layers.Dense(512, activation='relu'),
    k.layers.Dropout(0.3),

    k.layers.Dense(256, activation='relu'),
    k.layers.Dropout(0.3),

    k.layers.Dense(128, activation='relu'),
    k.layers.Dropout(0.3),

    k.layers.Dense(64, activation='relu'),
    k.layers.Dropout(0.3),

    k.layers.Dense(10, activation='softmax'),
])

print(model_4nn.summary())
model_4nn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])
history_4nn = model_4nn.fit(X_train, y_train, epochs=150, batch_size=72, 
                  validation_data = [X_test,y_test],shuffle=False)

fig = go.Figure()
fig.add_trace(go.Scatter(y=history_4nn.history['acc'], x=[i for i in range(0,50)],
                    mode='lines',
                    name='Training Accuracy'))
fig.add_trace(go.Scatter(y=history_4nn.history['val_acc'], x=[i for i in range(0,50)],
                    mode='lines',
                    name='Validation Accuracy'))
fig.update_layout(title='Validation and Training Accuracies',
                   xaxis_title='epochs',
                   yaxis_title='Accuracy')
fig.show()

fig = go.Figure()
fig.add_trace(go.Scatter(y=history_4nn.history['loss'], x=[i for i in range(0,50)],
                    mode='lines',
                    name='Training Loss'))
fig.add_trace(go.Scatter(y=history_4nn.history['val_loss'], x=[i for i in range(0,50)],
                    mode='lines',
                    name='Validation Loss'))
fig.update_layout(title='Validation and Training Loss',
                   xaxis_title='loss',
                   yaxis_title='epochs')
fig.show()

"""## Ensemble Models"""

class ensemble():
  def __init__(self,model1,model2,model3,model4):
    self.model1 = model1
    self.model2 = model2
    self.model3 = model3
    self.model4 = model4
  
  def final(self,y1,y2,y3,y4):
    test_list = np.array([y1,y2,y3,y4])
    # print(test_list)
    res = max([p[0] for p in statistics._counts(test_list)])
    return res

  def predict(self,X):
    y1=list(self.model1.predict(X))
    y2=list(self.model2.predict(X))
    y3=list(self.model3.predict(X))
    y4=list(self.model4.predict(X))
    y = []
    
    for i in range(len(y1)):
      y.append(self.final(y1[i],y2[i],y3[i],y4[i]))
    return y

class priority_ensemble_classifier():
  def __init__(self,model1,model2,model3,model4,validation_scores):
    self.model1 = model1
    self.model2 = model2
    self.model3 = model3
    self.model4 = model4
    self.validation_scores=validation_scores
  
  def predict(self,X_test):
    self.probs1 = self.model1.predict_proba(X_test)*self.validation_scores[0]
    self.probs2 = self.model2.predict_proba(X_test)*self.validation_scores[1]
    self.probs3 = self.model3.predict_proba(X_test)*self.validation_scores[2]
    self.probs4 = self.model4.predict_proba(X_test)*self.validation_scores[3]
    final_probs = self.probs1+self.probs2+self.probs3+self.probs4
    print(np.shape(final_probs))
    y_pred = [np.argmax(i) for i in final_probs]
    return y_pred

model_priority = priority_ensemble_classifier(model1,model3,model4,model5,[0.75,0.72,0.72,0.78])
y_pred_priority = model_priority.predict(X_test)
accuracy_score(y_test,y_pred_priority)

model_stacking = ensemble(model1,model3,model4,model5)
y_pred_stacking = model_stacking.predict(X_test)
accuracy_score(y_test,y_pred_stacking)

"""##Pipeline Creation """



"""## Model Saving"""

joblib.dump(model_4nn, 'model.pkl')
model_load = joblib.load('model.pkl')
model_load.evaluate(X_test,y_test,batch_size=72)

joblib.dump(model5, 'model.pkl')
model_load = joblib.load('model.pkl')

"""## Unused Functions/Cells"""

def dict_extract_to_df(dataframe):
  col = ['chroma_stft', 'rms', 'spectral_centroid','spectral_bandwidth', 'spectral_rolloff', 'zero_crossing_rate','harmon', 'plp']
  X = []
  for i in col:
    mean = []
    var = []
    for j in dataframe[i]:
      mean.append(np.mean(j))
      var.append(np.var(j)) 
    X.append(mean)
    X.append(var)
  l = []
  for i in dataframe['tempo']:
    l.append(i[0])
  X.append(l)
  X = np.array(X)
  mfc = []
  for m in range(0,20):
    mean = []
    var = []
    for i in dataframe['mfcc']:
        var.append(np.var(i[m]))
        mean.append(np.mean(i[m]))
    mfc.append(mean)
    mfc.append(var)
  mfc = np.array(mfc)
  X = np.concatenate([X.T,mfc.T],axis = 1)
  dfcol = ['chroma_stft_mean', 'chroma_stft_var', 'rms_mean',
        'rms_var', 'spectral_centroid_mean', 'spectral_centroid_var',
        'spectral_bandwidth_mean', 'spectral_bandwidth_var', 'rolloff_mean',
        'rolloff_var', 'zero_crossing_rate_mean', 'zero_crossing_rate_var',
        'harmony_mean', 'harmony_var', 'plp_mean', 'plp_var', 'tempo','mfcc1_mean', 'mfcc1_var', 'mfcc2_mean', 'mfcc2_var', 'mfcc3_mean',
        'mfcc3_var', 'mfcc4_mean', 'mfcc4_var', 'mfcc5_mean', 'mfcc5_var',
        'mfcc6_mean', 'mfcc6_var', 'mfcc7_mean', 'mfcc7_var', 'mfcc8_mean',
        'mfcc8_var', 'mfcc9_mean', 'mfcc9_var', 'mfcc10_mean', 'mfcc10_var',
        'mfcc11_mean', 'mfcc11_var', 'mfcc12_mean', 'mfcc12_var', 'mfcc13_mean',
        'mfcc13_var', 'mfcc14_mean', 'mfcc14_var', 'mfcc15_mean', 'mfcc15_var',
        'mfcc16_mean', 'mfcc16_var', 'mfcc17_mean', 'mfcc17_var', 'mfcc18_mean',
        'mfcc18_var', 'mfcc19_mean', 'mfcc19_var', 'mfcc20_mean', 'mfcc20_var']
  df = pd.DataFrame(X,columns = dfcol)
  return df